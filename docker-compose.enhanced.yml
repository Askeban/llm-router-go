version: '3.8'

services:
  # Redis for caching and performance optimizations
  redis:
    image: redis:7-alpine
    container_name: llm-router-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - llm-router-network

  # Enhanced Classifier Service
  classifier:
    build:
      context: ./python/classifier_service
      dockerfile: Dockerfile
    container_name: llm-router-classifier
    ports:
      - "5001:5000"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CACHE_TTL=3600
      - LOG_LEVEL=INFO
      - CURL_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt
      - REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt
      - SSL_CERT_FILE=/etc/ssl/certs/ca-certificates.crt
      - SSL_CERT_DIR=/etc/ssl/certs
      - SSL_VERIFY=false
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      - classifier_cache:/app/semantic_cache
      - classifier_rules:/app/rules_cache
      - classifier_feedback:/app/feedback_cache
    networks:
      - llm-router-network
    restart: unless-stopped

  # Enhanced Ingestor Service
  ingestor:
    build:
      context: ./python/ingestor_service
      dockerfile: Dockerfile
    container_name: llm-router-ingestor
    ports:
      - "8001:8001"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - ANALYTICS_API_KEY=${ANALYTICS_API_KEY:-}
      - MODELS_JSON_PATH=/app/data/models.json
      - SCRAPED_DATA_DIR=/app/data/scraped
      - OUTPUT_PATH=/app/data/enhanced_models.json
      - ENABLE_FEEDBACK=true
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      - ingestor_data:/app/data
      - ingestor_cache:/app/cache
      - ingestor_backups:/app/backups
      - ./configs:/app/data/scraped:ro
      - ./internal/models.json:/app/data/models.json:ro
    networks:
      - llm-router-network
    restart: unless-stopped

  # Main Router Service (Go)
  router:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm-router-main
    ports:
      - "8080:8080"
    environment:
      - CLASSIFIER_URL=http://classifier:5000
      - DATABASE_PATH=/data/router.db
      - MODEL_PROFILES_PATH=/root/internal/models.json
      - PORT=8080
      - GIN_MODE=release
    depends_on:
      - classifier
      - ingestor
      - redis
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    volumes:
      - router_data:/data
      - ./configs:/configs:ro
    networks:
      - llm-router-network
    restart: unless-stopped

  # Optional: Monitoring with simple HTTP health dashboard
  health-dashboard:
    image: nginx:alpine
    container_name: llm-router-dashboard
    ports:
      - "8090:80"
    volumes:
      - ./docker/health-dashboard.html:/usr/share/nginx/html/index.html:ro
    depends_on:
      - router
      - classifier
      - ingestor
    networks:
      - llm-router-network
    restart: unless-stopped

volumes:
  redis_data:
  router_data:
  ingestor_data:
  ingestor_cache:
  ingestor_backups:
  classifier_cache:
  classifier_rules:
  classifier_feedback:

networks:
  llm-router-network:
    driver: bridge