version: '3.8'

services:
  # Redis for caching and performance optimizations
  redis:
    image: redis:7-alpine
    container_name: llm-router-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - llm-router-network
    restart: unless-stopped

  # Main API Service
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    container_name: llm-router-api
    ports:
      - "8080:8080"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - CLASSIFIER_URL=http://classifier:5000
      - SQLITE_PATH=/app/data/router.db
      - MODEL_PROFILES_PATH=/app/configs/models.json
      - ANALYTICS_AI_KEY=${ANALYTICS_AI_KEY:-aa_hvPVoBMuwefckQlniBWCrpQUmPdNSift}
      - CLASSIFIER_RULES_PATH=/app/configs/classifier_rules.json
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - classifier
      - ingestor
      - redis
    volumes:
      - api_data:/app/data
      - ./configs:/app/configs:ro
    networks:
      - llm-router-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # Enhanced Classifier Service
  classifier:
    build:
      context: .
      dockerfile: docker/Dockerfile.classifier
    container_name: llm-router-classifier
    ports:
      - "5002:5000"
    environment:
      - CLASSIFIER_RULES_PATH=/app/configs/classifier_rules.json
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - CACHE_TTL=3600
      - LOG_LEVEL=INFO
    depends_on:
      - redis
    volumes:
      - ./configs:/app/configs:ro
      - classifier_cache:/app/cache
    networks:
      - llm-router-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Enhanced Ingestor Service
  ingestor:
    build:
      context: ./python/ingestor_service
      dockerfile: Dockerfile.enhanced
    container_name: llm-router-ingestor
    ports:
      - "8001:8001"
    environment:
      - ROUTER_API=http://api:8080/ingest
      - SOURCES=open-llm-leaderboard,lmarena,artificialanalysis
      - ANALYTICS_API_KEY=${ANALYTICS_API_KEY:-aa_hvPVoBMuwefckQlniBWCrpQUmPdNSift}
      - HELM_GCS_BUCKET=crfm-helm-public
      - HELM_GCS_PREFIX=lite/benchmark_output/runs
      - HELM_GCS_SUITE=v1.0.0
      - HELM_GCS_LIMIT=50
      - HELM_JSON_URLS=${HELM_JSON_URLS}
      - MODELS_JSON_PATH=/app/data/models.json
      - SCRAPED_DATA_DIR=/app/configs
      - OUTPUT_PATH=/app/data/enhanced_models.json
      - PORT=8001
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    depends_on:
      - redis
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    volumes:
      - ingestor_data:/app/data
      - ingestor_cache:/app/cache
      - ingestor_backups:/app/backups
      - ./configs:/app/configs:ro
      - ./internal/models.json:/app/data/models.json:ro
    networks:
      - llm-router-network
    restart: unless-stopped

  # Health Dashboard
  health-dashboard:
    image: nginx:alpine
    container_name: llm-router-dashboard
    ports:
      - "8090:80"
    volumes:
      - ./docker/health-dashboard.html:/usr/share/nginx/html/index.html:ro
    depends_on:
      - api
      - classifier
      - ingestor
    networks:
      - llm-router-network
    restart: unless-stopped

volumes:
  redis_data:
  api_data:
  ingestor_data:
  ingestor_cache:
  ingestor_backups:
  classifier_cache:

networks:
  llm-router-network:
    driver: bridge